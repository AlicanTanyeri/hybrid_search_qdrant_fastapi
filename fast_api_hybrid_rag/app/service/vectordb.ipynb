{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42941e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_119197/2716592562.py:6: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")  # Qdrant endpointini girin\n",
    "\n",
    "collection_name = \"hybrid_search\"\n",
    "client.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config={\n",
    "        \"dense\": models.VectorParams(size=384, distance=models.Distance.COSINE),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"sparse\": models.SparseVectorParams(\n",
    "            index=models.SparseIndexParams(on_disk=True)\n",
    "        )\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcff1ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"umarigan/turkish_wikipedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fc5a6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "wiki_data = ds[\"train\"].select(range(0, 10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2299db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'text', 'title'],\n",
      "    num_rows: 10000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31e2a7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "id = list(wiki_data[\"id\"])\n",
    "title = list(wiki_data[\"title\"])\n",
    "text = list(wiki_data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328a85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "cleaned_texts = [clean_text(t) for t in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b75cba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film şu anlamlara gelebilir camlara yapıştırılarak içerinin görünmesini engelleyen bir tür ince yaprak sinemacılıkta bir oyunun bütününü taşıyan şerit veya şeritlerin bütünü film fotoğrafçılık fotoğrafçılıkta radyografide ve sinemacılıkta resim çekmek için kullanılan selülozdan saydam bükülebilir şerit film sinema sinema makinesiyle gösterilen eser izleti film film samuel beckett in yazdığı tek senaryodan çekilen 1965 abd yapımı film total film 1997 den beri i ngiltere de yayımlanan sinema dergisi\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30358c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tiktoken in /home/alicantanyeri/.local/lib/python3.13/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/lib64/python3.13/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/lib/python3.13/site-packages (from tiktoken) (2.32.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3.13/site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05b2fa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import re\n",
    "\n",
    "def chunk_text(text, max_tokens=128):\n",
    "    \"\"\"\n",
    "    Split text into chunks of maximum 128 tokens.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to chunk\n",
    "        max_tokens (int): Maximum tokens per chunk (default: 128)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of text chunks\n",
    "    \"\"\"\n",
    "    # Initialize tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # If text is already small enough, return as is\n",
    "    if len(tokenizer.encode(text)) <= max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    # Split by sentences first\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Test if adding this sentence exceeds limit\n",
    "        test_chunk = current_chunk + (\" \" if current_chunk else \"\") + sentence\n",
    "        \n",
    "        if len(tokenizer.encode(test_chunk)) <= max_tokens:\n",
    "            current_chunk = test_chunk\n",
    "        else:\n",
    "            # Save current chunk if it has content\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            \n",
    "            # Start new chunk with current sentence\n",
    "            # If single sentence is too long, split by words\n",
    "            if len(tokenizer.encode(sentence)) > max_tokens:\n",
    "                words = sentence.split()\n",
    "                temp_chunk = \"\"\n",
    "                \n",
    "                for word in words:\n",
    "                    test_word_chunk = temp_chunk + (\" \" if temp_chunk else \"\") + word\n",
    "                    \n",
    "                    if len(tokenizer.encode(test_word_chunk)) <= max_tokens:\n",
    "                        temp_chunk = test_word_chunk\n",
    "                    else:\n",
    "                        if temp_chunk:\n",
    "                            chunks.append(temp_chunk.strip())\n",
    "                        temp_chunk = word\n",
    "                \n",
    "                current_chunk = temp_chunk\n",
    "            else:\n",
    "                current_chunk = sentence\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_dataset(dataset, max_tokens=128):\n",
    "    \"\"\"\n",
    "    Chunk a list of texts.\n",
    "    \n",
    "    Args:\n",
    "        dataset (list): List of text paragraphs\n",
    "        max_tokens (int): Maximum tokens per chunk\n",
    "    \n",
    "    Returns:\n",
    "        list: All chunks from all paragraphs\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "    for text in dataset:\n",
    "        chunks = chunk_text(text, max_tokens)\n",
    "        all_chunks.extend(chunks)\n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0b76d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts=chunk_dataset(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "879d32a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "film şu anlamlara gelebilir camlara yapıştırılarak içerinin görünmesini engelleyen bir tür ince yaprak sinemacılıkta bir oyunun bütününü taşıyan şerit veya şeritlerin bütünü film fotoğrafçılık fotoğrafçılıkta radyografide ve sinemacılıkta resim çekmek için kullanılan selülozdan saydam bükülebilir şerit film sinema sinema\n",
      "makinesiyle gösterilen eser izleti film film samuel beckett in yazdığı tek senaryodan çekilen 1965 abd yapımı film total film 1997 den beri i ngiltere de yayımlanan sinema dergisi\n",
      "bolşevik çoğunluktan yana anlamına gelen rusça kelime 1903 yılında düzenlenen rusya sosyal demokrat i şçi partisi nin i kinci kongresi nde vladimir lenin ve julius martov arasında yeni kurulmakta olan partinin üyelik tanımı üzerine başlayan görüş ayrılığı sonucu yaşanan ayrışmadaki taraflardan lenin yanlısı grup kongrede lenin yanlıları\n",
      "çoğunlukta olduğu için rusça çoğunluk anlamına gelen bolşevik olarak azınlıktaki martov yanlıları da menşevik olarak adlandırılacaktır kongreden sonra iki taraf arasında birleşme girişimleri olsa da birleşme gerçekleşmeyecek ve 1912 yılında kesin ayrım yaşanacaktır bolşevikler ekim devrimi ile iktidarı alacaklar ve sovyetler\n",
      "birliği ni kuracaklardır bölünmenin tarihçesi polis baskısı nedeniyle önce brüksel sonra da londra da yapılan rusya sosyal demokrat i şçi partisi rsdi p 2 kongresi 1903 yılı ağustos ayında toplanır toplantıda yeni partinin üyelik esasları ve tanımı üzerinden önemli bir ayrılma yaşanacak ve rusya daki devrimci hareketi derinden etkileyecektir\n",
      "vladimir i lyiç ulyanov ya da takma adıyla lenin parti üyelerinin dar ve aktif bir çevreden oluşmasını sadece ceplerinde parti kimliği taşıyan ve zaman zaman partiye uğrayanlardan hatta hiç uğramayanlardan oluşmamasını savunuyordu bu faal üyeler profesyonel devrimci kadrolar olarak çarlık otokrasisine karşı işçi devrimi yapabilecek bir devrimci partinin\n",
      "yaratılabilmesi için zamanlarının çoğunu örgütlenmeyle geçireceklerdir bu modele göre sempatizanlar dışarıda bırakılmış olmaktaydı partinin iç işleyişinde de demokratik merkeziyetçilik benimsenecekti lenin in bu fikirlerine karşıt olarak ise arkadaşı julius martov partinin merkezinde profesyonel devrimcilerin olmasına onay verse de parti üyeliğinin sempatizanlara\n",
      "devrimci işçilere ve diğerlerine açık olmasını savunuyordu i kili bu konuyu daha önce de tartışmış olsalar da görüş ayrılıkları kongrede ayrılığa yol açacaktır ayrılık ufak bir konuda ve kişisel ayrımdan kaynaklanıyor görünse de ayrım derinleşecek ve bölünme kaçınılmaz hale gelecektir i smin kökeni lenin ve martov\n",
      "yandaşları kongredeki durumlarına göre rusça bolshinstvo çoğunluk ve menshinstvo azınlık olarak adlandırılırlarkongre lenin in savunduğu parti üyelik tarzını benimser kongredeki delegeler sürekli olarak saf değiştirdikleri için birleşim başarısız olacak ve parti fiilen ikiye bölünecektir 1905 devrimi i ki taraf da sürekli olarak yeni üyeler kazanıyor ve\n",
      "kaybediyordu rus marksizminin babası olarak adlandırılan georgi plehanov ilk başta lenin ve bolşeviklerden yana olacak ancak 1904 te ayrılacaktır menşeviklerden yana olan leon troçki ise menşeviklerin rus liberalleriyle uzlaşma girişimleri ve bolşeviklerle birleşmeme tutumları yüzünden ayrılacaktırtroçki ve yakın arkadaşları\n",
      "bolşeviklere dahil olmasa da onlara çok yakın duracaklar ve enternasyonalistler olarak adlndırılan grup ağustos 1917 de bolşeviklere katılacaklardır i ki taraf arasındaki ayrım nisan 1905 te bolşeviklerin ayrı yaptığı ve 3 kongre olarak adlandıracakları kongre ile derinleşecektir menşevikler derhal alternatif bir kongre yapacaklardır rus i\n"
     ]
    }
   ],
   "source": [
    "for i in range (0,11):\n",
    "    print(cleaned_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b8bfab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alicantanyeri/.local/lib/python3.13/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/home/alicantanyeri/.local/lib/python3.13/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Batches: 100%|██████████| 1437/1437 [13:14<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "texts = cleaned_texts\n",
    "dense_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', device=\"cuda\")\n",
    "dense_vectors = dense_model.encode(texts, show_progress_bar=True, batch_size=128).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb5b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_sparse_matrix = tfidf.fit_transform(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0928ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2fcb033",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sparse_vec(sparse_row):\n",
    "    indices = sparse_row.indices.tolist()\n",
    "    values = sparse_row.data.tolist()\n",
    "    return {\"indices\": indices, \"values\": values}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "534a11ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 - 250 yüklendi.\n",
      "Batch 250 - 500 yüklendi.\n",
      "Batch 500 - 750 yüklendi.\n",
      "Batch 750 - 1000 yüklendi.\n",
      "Batch 1000 - 1250 yüklendi.\n",
      "Batch 1250 - 1500 yüklendi.\n",
      "Batch 1500 - 1750 yüklendi.\n",
      "Batch 1750 - 2000 yüklendi.\n",
      "Batch 2000 - 2250 yüklendi.\n",
      "Batch 2250 - 2500 yüklendi.\n",
      "Batch 2500 - 2750 yüklendi.\n",
      "Batch 2750 - 3000 yüklendi.\n",
      "Batch 3000 - 3250 yüklendi.\n",
      "Batch 3250 - 3500 yüklendi.\n",
      "Batch 3500 - 3750 yüklendi.\n",
      "Batch 3750 - 4000 yüklendi.\n",
      "Batch 4000 - 4250 yüklendi.\n",
      "Batch 4250 - 4500 yüklendi.\n",
      "Batch 4500 - 4750 yüklendi.\n",
      "Batch 4750 - 5000 yüklendi.\n",
      "Batch 5000 - 5250 yüklendi.\n",
      "Batch 5250 - 5500 yüklendi.\n",
      "Batch 5500 - 5750 yüklendi.\n",
      "Batch 5750 - 6000 yüklendi.\n",
      "Batch 6000 - 6250 yüklendi.\n",
      "Batch 6250 - 6500 yüklendi.\n",
      "Batch 6500 - 6750 yüklendi.\n",
      "Batch 6750 - 7000 yüklendi.\n",
      "Batch 7000 - 7250 yüklendi.\n",
      "Batch 7250 - 7500 yüklendi.\n",
      "Batch 7500 - 7750 yüklendi.\n",
      "Batch 7750 - 8000 yüklendi.\n",
      "Batch 8000 - 8250 yüklendi.\n",
      "Batch 8250 - 8500 yüklendi.\n",
      "Batch 8500 - 8750 yüklendi.\n",
      "Batch 8750 - 9000 yüklendi.\n",
      "Batch 9000 - 9250 yüklendi.\n",
      "Batch 9250 - 9500 yüklendi.\n",
      "Batch 9500 - 9750 yüklendi.\n",
      "Batch 9750 - 10000 yüklendi.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     12\u001b[39m     sparse_dict = to_sparse_vec(tfidf_sparse_matrix[idx])\n\u001b[32m     13\u001b[39m     sparse_vec = SparseVector(indices=sparse_dict[\u001b[33m\"\u001b[39m\u001b[33mindices\u001b[39m\u001b[33m\"\u001b[39m], values=sparse_dict[\u001b[33m\"\u001b[39m\u001b[33mvalues\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     15\u001b[39m     batch_points.append(\n\u001b[32m     16\u001b[39m         PointStruct(\n\u001b[32m     17\u001b[39m             \u001b[38;5;28mid\u001b[39m=idx,\n\u001b[32m     18\u001b[39m             vector={\n\u001b[32m     19\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mdense\u001b[39m\u001b[33m\"\u001b[39m: dense_vec,\n\u001b[32m     20\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msparse\u001b[39m\u001b[33m\"\u001b[39m: sparse_vec\n\u001b[32m     21\u001b[39m             },\n\u001b[32m     22\u001b[39m             payload={\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mid\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[32m     24\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: wiki_data[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m][idx],\n\u001b[32m     25\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m: title[idx]}\n\u001b[32m     26\u001b[39m         )\n\u001b[32m     27\u001b[39m     )\n\u001b[32m     29\u001b[39m client.upsert(collection_name=collection_name, points=batch_points)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstart_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m yüklendi.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "from qdrant_client.models import PointStruct, SparseVector\n",
    "\n",
    "batch_size = 250  \n",
    "points = []\n",
    "\n",
    "for start_idx in range(0, len(texts), batch_size):\n",
    "    end_idx = min(start_idx + batch_size, len(texts))\n",
    "\n",
    "    batch_points = []\n",
    "    for idx in range(start_idx, end_idx):\n",
    "        dense_vec = dense_vectors[idx]\n",
    "        sparse_dict = to_sparse_vec(tfidf_sparse_matrix[idx])\n",
    "        sparse_vec = SparseVector(indices=sparse_dict[\"indices\"], values=sparse_dict[\"values\"])\n",
    "\n",
    "        batch_points.append(\n",
    "            PointStruct(\n",
    "                id=idx,\n",
    "                vector={\n",
    "                    \"dense\": dense_vec,\n",
    "                    \"sparse\": sparse_vec\n",
    "                },\n",
    "                payload={\n",
    "                    \"id\": id[idx],\n",
    "                    \"text\": wiki_data[\"text\"][idx],\n",
    "                    \"title\": title[idx]}\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    client.upsert(collection_name=collection_name, points=batch_points)\n",
    "    print(f\"Batch {start_idx} - {end_idx} yüklendi.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
